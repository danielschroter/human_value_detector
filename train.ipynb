{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePVSga5E_a-e"
   },
   "source": [
    "# Train a Language Model to Detect Human Values in Arguments\n",
    "\n",
    "The following notebook contains the training-procedure for a single training.\n",
    "The final Model is an ensemble of several such runs. More information can be found in the system description paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25237,
     "status": "ok",
     "timestamp": 1667235617561,
     "user": {
      "displayName": "Daniel Schroter",
      "userId": "14162089145838719475"
     },
     "user_tz": -60
    },
    "id": "f3tx1knExlud",
    "outputId": "e08abd38-93af-466d-a856-e1bff8980834"
   },
   "outputs": [],
   "source": [
    "# if on google colab\n",
    "!pip install -q pytorch-lightning==1.6.4 neptune-client transformers sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10254,
     "status": "ok",
     "timestamp": 1667235627811,
     "user": {
      "displayName": "Daniel Schroter",
      "userId": "14162089145838719475"
     },
     "user_tz": -60
    },
    "id": "y5mj8cpdM_O3",
    "outputId": "d8cefe5e-05ab-4ca4-fa7c-aa25d9f2c024"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": "42"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import NeptuneLogger\n",
    "\n",
    "from torchmetrics import AUROC\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import torch.nn as nn\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "\n",
    "\n",
    "pl.seed_everything(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btZjckeJs5j7"
   },
   "source": [
    "If you are training to google colab and want to connect to drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 245,
     "status": "ok",
     "timestamp": 1667235666818,
     "user": {
      "displayName": "Daniel Schroter",
      "userId": "14162089145838719475"
     },
     "user_tz": -60
    },
    "id": "UuJkkr9Mpa3h",
    "outputId": "8e00bb43-f22e-4192-be3e-4ee617634117"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17685,
     "status": "ok",
     "timestamp": 1667235684811,
     "user": {
      "displayName": "Daniel Schroter",
      "userId": "14162089145838719475"
     },
     "user_tz": -60
    },
    "id": "QE5P0dCepbkD",
    "outputId": "e41c97be-adda-4154-b24d-413ae994af5a"
   },
   "outputs": [],
   "source": [
    "cd ./drive/MyDrive/human_value/human_values_behind_arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules\n",
    "We use Pytorch Lightning for the training and therefore import the Lighntning Data and Model Modules...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_modules.BertDataModule import BertDataModule, BertDataset\n",
    "from models.BertFineTunerPl import BertFineTunerPl\n",
    "from weights.weights import INS #Weights for Weighting Loss Function (optional)\n",
    "from toolbox.bert_utils import max_for_thres # Algorithm that chooses threshold that maximizes f1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    # Language Model and Hyperparameters\n",
    "    \"MODEL_PATH\": 'roberta-base',\n",
    "    \"BATCH_SIZE\": 8,\n",
    "    \"ACCUMULATE_GRAD_BATCHES\": 1,\n",
    "    \"LR\": 2e-5,\n",
    "    \"EPOCHS\": 3,\n",
    "    \"OPTIMIZER\": 'AdamW',\n",
    "    \"DEVICE\": torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    \"NUM_TRAIN_WORKERS\": 4,\n",
    "    \"NUM_VAL_WORKERS\": 4,\n",
    "    \"MAX_TOKEN_COUNT\":165,\n",
    "    \"RANDOM_SEED\": RANDOM_SEED, #Random Seed Selected for this Training Run\n",
    "\n",
    "    # Apply Weights to loss function (optional, the submitted system does not weight the loss function).\n",
    "    \"WEIGHTS\": INS,\n",
    "    \"CRITERION\": [nn.BCEWithLogitsLoss()],\n",
    "    # \"CRITERION\": [nn.BCEWithLogitsLoss(pos_weight=torch.Tensor(INS))], # Optional\n",
    "\n",
    "\n",
    "\n",
    "    # Early Stopping Params\n",
    "    \"PATIENCE\": 3,\n",
    "    \"VAL_CHECK_INTERVAL\": 300,\n",
    "\n",
    "    # alternative \"custom_f1/Val\" and \"max\"\n",
    "    \"MAX_THRESHOLD_METRIC\": \"custom\", #The f1-score that should maximized (custom = formula for the task evaluation)\n",
    "    \"EARLY_STOPPING_METRIC\": \"avg_val_loss\",\n",
    "    \"EARLY_STOPPING_MODE\": \"min\",\n",
    "\n",
    "    # Additional Dropout or Additional Hidden Layers (Not used for the final submission)\n",
    "    \"DROPOUT\": None, # e.g 0.5 (float)\n",
    "    \"HIDDEN_LAYERS\":None, # Of Shape [(512, nn.ReLU()),...] put size of hidden Layer together with activation function in list\n",
    "\n",
    "    # ONLY CHANGE TOGETHER\n",
    "    \"VALIDATION_SET_SIZE\":500,\n",
    "    # \"TEST_SET_SIZE\": 500,\n",
    "\n",
    "    \"EMBEDDING\": \"CLS\", # \"CLS + MEAN\" for both. Which information should be used from Bert-Output. CLS Token in Submission.\n",
    "\n",
    "    \"TRAIN_PATH\" : \"./data/data_training_full.csv\", #\n",
    "    \"LEAVE_OUT_DATA_PATH\": \"./data/leave_out_dataset_300.csv\"\n",
    "    # \"VALIDATION_PATH\" : None,\n",
    "    # \"TEST_PATH\" : \"data_test_individual_v2_500.csv\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preprocessing\n",
    "Please see the data_gen.ipynb notebook... We create the training-data, and leave-out-datafiles there and save them in the data directory.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(PARAMS[\"TRAIN_PATH\"], index_col=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now get the LABEL_COLUMNS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "LABEL_COLUMNS = train_df.columns.tolist()[6:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the best-performing submission we used a Leave-Out-Dataset, to determine the optimal threshold that maximizes the f1-score at the end. This dataset is used to determine the best threshold for an ensembled model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "leave_out_df = pd.read_csv(PARAMS[\"LEAVE_OUT_DATA_PATH\"], index_col=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (OPTIONAL) Create optional test dataset if you would like to evaluate the performance of the model immediately"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(train_df, test_size=500, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSQsvaBgoM7F"
   },
   "source": [
    "### Linear Learning Rate Schedule\n",
    "Define Parameters for the Linear Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "r2ywIz841yni"
   },
   "outputs": [],
   "source": [
    "steps_per_epoch=len(train_df) // PARAMS['BATCH_SIZE']\n",
    "total_training_steps = steps_per_epoch * PARAMS['EPOCHS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjV_biJ72ERg"
   },
   "source": [
    "We'll use a fifth of the training steps for a warm-up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1667223230416,
     "user": {
      "displayName": "Daniel Schroter",
      "userId": "14162089145838719475"
     },
     "user_tz": -60
    },
    "id": "OSHJ3V47G90d",
    "outputId": "f096a726-b8f3-406a-9078-6ec5780fc3aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(515, 2577)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warmup_steps = total_training_steps // 5\n",
    "warmup_steps, total_training_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prepare Data Modules for the Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Validation Set for this Run\n",
    "To make use of the total available data, the models in the final ensemble are trained on different train-validation splits..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(train_df, test_size=PARAMS[\"VALIDATION_SET_SIZE\"], random_state=PARAMS[\"RANDOM_SEED\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create Tokenizer and Data Module for the Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = AutoTokenizer.from_pretrained(PARAMS[\"MODEL_PATH\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = BertDataModule(\n",
    "    train_df,\n",
    "    val_df,\n",
    "    tokenizer=TOKENIZER,\n",
    "    params=PARAMS,\n",
    "    label_columns=LABEL_COLUMNS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertFineTunerPl(n_classes=len(LABEL_COLUMNS), params=PARAMS, label_columns=LABEL_COLUMNS, n_training_steps=total_training_steps, n_warmup_steps=warmup_steps)\n",
    "NAME = f\"{PARAMS['MODEL_PATH'].replace('/','-')}-BS_{PARAMS['BATCH_SIZE']}-LR_{PARAMS['LR']}-HL_{PARAMS['HIDDEN_LAYERS']}-DROPOUT_{PARAMS['DROPOUT']}\"\n",
    "RUN_ID = None"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We use Neptune.ai to log the experiment. This is optional. You need to create a new project and add the project and api key for the neptune logger."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Optional: Log Experiments to Neptune\n",
    "\n",
    "neptune_logger = NeptuneLogger(\n",
    "    project=\"Your Project ID\", #\n",
    "    api_key=\"Your API KEY\", #\n",
    "    name=NAME,\n",
    "    tags=[f\"{PARAMS['MODEL_PATH']}\",f\"BS-{PARAMS['BATCH_SIZE']}\",f\"LR-{PARAMS['LR']}\",f\"LR-{PARAMS['HIDDEN_LAYERS']}\"],\n",
    "    log_model_checkpoints=False\n",
    ")\n",
    "neptune_logger.log_hyperparams(PARAMS)\n",
    "\n",
    "neptune_logger.experiment[\"train_size\"].log(len(train_df))\n",
    "neptune_logger.experiment[\"val_size\"].log(len(val_df))\n",
    "RUN_ID = neptune_logger._run_short_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Callbacks and create Pytorch Lightning Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Running in fast_dev_run mode: will run a full train, val, test and prediction loop using 1 batch(es).\n",
      "`Trainer(limit_train_batches=1)` was configured so 1 batch per epoch will be used.\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "`Trainer(limit_test_batches=1)` was configured so 1 batch will be used.\n",
      "`Trainer(limit_predict_batches=1)` was configured so 1 batch will be used.\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints\",\n",
    "    filename= (f\"{RUN_ID}-{NAME}\" if RUN_ID else f\"{NAME}\"),\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=PARAMS[\"EARLY_STOPPING_METRIC\"],\n",
    "    mode=PARAMS[\"EARLY_STOPPING_MODE\"]\n",
    ")\n",
    "early_stopping_callback = EarlyStopping(monitor=PARAMS[\"EARLY_STOPPING_METRIC\"], patience=PARAMS[\"PATIENCE\"], mode=PARAMS[\"EARLY_STOPPING_MODE\"])\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger=([neptune_logger] if RUN_ID else []),\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "    max_epochs=PARAMS[\"EPOCHS\"],\n",
    "    fast_dev_run=True,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    enable_progress_bar=True,\n",
    "    val_check_interval=PARAMS[\"VAL_CHECK_INTERVAL\"],\n",
    "    accumulate_grad_batches=PARAMS[\"ACCUMULATE_GRAD_BATCHES\"],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dschr\\anaconda3\\envs\\nlp_torch\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\dschr\\DataspellProjects\\human_value_detector\\checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type              | Params\n",
      "----------------------------------------------------\n",
      "0 | bert          | RobertaModel      | 124 M \n",
      "1 | hidden_layers | ModuleList        | 0     \n",
      "2 | classifier    | Linear            | 15.4 K\n",
      "3 | criterion     | BCEWithLogitsLoss | 0     \n",
      "----------------------------------------------------\n",
      "124 M     Trainable params\n",
      "0         Non-trainable params\n",
      "124 M     Total params\n",
      "498.644   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fe0808361f504562bed400142a37d4a2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9db05f0b77dc4277866bf0f10b05b66e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dschr\\anaconda3\\envs\\nlp_torch\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data_module)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "if RUN_ID:\n",
    "    with open(f\"./checkpoints/{RUN_ID}_PARAMS.pkl\", 'wb') as f:\n",
    "        pickle.dump(PARAMS, f)\n",
    "else:\n",
    "    with open(f\"./checkpoints/{NAME}_PARAMS.pkl\", 'wb') as f:\n",
    "        pickle.dump(PARAMS, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# If Logging (optional)\n",
    "neptune_logger.experiment[\"best_model_checkpoint\"].log(trainer.checkpoint_callback.best_model_path)\n",
    "neptune_logger.log_model_summary(model=model, max_depth=-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we are done with the training. This process is repeated with several different configurations for the model. More information can be found in the system description paper."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation\n",
    "\n",
    "The predictions for the final submissions are done based on an ensemble.\n",
    "Hence for ensembling, please continue with the ensemble_eval_and_predict.ipynb notebook.\n",
    "However, for simplicity or if you are interested, you may want to continue here to evaluate the model performance.\n",
    "\n",
    "1. We determine the decision threshold to decide when a certain label should be counted as 1, based on the val_data\n",
    "2. We predict the test_data with it (if splitted above)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We load the model from the best_checkpoint in order to get the model that performed best with respect to the early stopping metric."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trained_model = BertFineTunerPl.load_from_checkpoint(\n",
    "    trainer.checkpoint_callback.best_model_path,\n",
    "    params=PARAMS,\n",
    "    label_columns=LABEL_COLUMNS,\n",
    "    n_classes=len(LABEL_COLUMNS)\n",
    ")\n",
    "\n",
    "trained_model.eval()\n",
    "trained_model.freeze()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We get the predictions for the val_df."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/500 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5a3e04f3307d49b9bb126199c8beac4c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "trained_model = trained_model.to(device)\n",
    "\n",
    "val_dataset = BertDataset(\n",
    "    val_df,\n",
    "    tokenizer=TOKENIZER,\n",
    "    max_token_count=PARAMS[\"MAX_TOKEN_COUNT\"],\n",
    "    label_columns=LABEL_COLUMNS\n",
    ")\n",
    "\n",
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "for item in tqdm(val_dataset):\n",
    "    _, prediction = trained_model(\n",
    "        item[\"input_ids\"].unsqueeze(dim=0).to(device),\n",
    "        item[\"attention_mask\"].unsqueeze(dim=0).to(device)\n",
    "    )\n",
    "    predictions.append(prediction.flatten())\n",
    "    labels.append(item[\"labels\"].int())\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Select optimal Threshold on Val Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "predictions = torch.stack(predictions).detach().cpu()\n",
    "labels = torch.stack(labels).detach().cpu()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "from toolbox.bert_utils import max_for_thres"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "THRESHOLD = max_for_thres(y_pred=predictions, y_true=labels, label_columns=LABEL_COLUMNS, average=PARAMS[\"MAX_THRESHOLD_METRIC\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Alternatively if you just one to load a model from checkpoint"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "with open(f'./checkpoints/HCV-409_PARAMS.pkl', 'rb') as f:\n",
    "    loaded_dict = pickle.load(f)\n",
    "    PARAMS = loaded_dict\n",
    "\n",
    "trained_model = BertFineTunerPl.load_from_checkpoint(\n",
    "    \"./checkpoints/HCV-409-microsoft-deberta-large-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt\",\n",
    "    params=PARAMS,\n",
    "    label_columns=LABEL_COLUMNS,\n",
    "    n_classes=len(LABEL_COLUMNS)\n",
    ")\n",
    "\n",
    "trained_model.eval()\n",
    "trained_model.freeze()\n",
    "\n",
    "THRESHOLD = 0.25"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "binarize the predictions with the optimal threshold"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred = predictions.numpy()\n",
    "y_true = labels.numpy()\n",
    "\n",
    "upper, lower = 1, 0\n",
    "\n",
    "y_pred = np.where(y_pred > THRESHOLD, upper, lower)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Threshold: {THRESHOLD}\")\n",
    "print(classification_report(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    target_names=LABEL_COLUMNS,\n",
    "    zero_division=0,\n",
    "))\n",
    "\n",
    "class_rep = classification_report(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    target_names=LABEL_COLUMNS,\n",
    "    zero_division=0,\n",
    "    output_dict=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Use Threshold to predict on Test Data\n",
    "If we want to predict on the test-data (if you have split it apart, alternatively you could use the leave-out-dataset). For a single Model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "test_df = leave_out_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/300 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a6b17a8ac0b8487bbcdad3c40149bf39"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "trained_model = trained_model.to(device)\n",
    "\n",
    "test_dataset = BertDataset(\n",
    "    test_df,\n",
    "    tokenizer=TOKENIZER,\n",
    "    max_token_count=PARAMS[\"MAX_TOKEN_COUNT\"],\n",
    "    label_columns=LABEL_COLUMNS\n",
    ")\n",
    "\n",
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "for item in tqdm(test_dataset):\n",
    "    _, prediction = trained_model(\n",
    "        item[\"input_ids\"].unsqueeze(dim=0).to(device),\n",
    "        item[\"attention_mask\"].unsqueeze(dim=0).to(device)\n",
    "    )\n",
    "    predictions.append(prediction.flatten())\n",
    "    labels.append(item[\"labels\"].int())\n",
    "\n",
    "\n",
    "predictions = torch.stack(predictions).detach().cpu()\n",
    "labels = torch.stack(labels).detach().cpu()\n",
    "\n",
    "\n",
    "y_pred = predictions.numpy()\n",
    "y_true = labels.numpy()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  Binarize the model predictions with Threshold\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "upper, lower = 1, 0\n",
    "\n",
    "y_pred = np.where(y_pred > THRESHOLD, upper, lower)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.25\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "   Self-direction: thought       0.41      0.92      0.57        49\n",
      "    Self-direction: action       0.69      0.89      0.78        75\n",
      "               Stimulation       0.19      0.64      0.29        14\n",
      "                  Hedonism       0.20      1.00      0.33         5\n",
      "               Achievement       0.74      0.92      0.82        86\n",
      "          Power: dominance       0.39      0.91      0.55        33\n",
      "          Power: resources       0.50      1.00      0.67        26\n",
      "                      Face       0.24      0.56      0.34        25\n",
      "        Security: personal       0.71      0.95      0.81       103\n",
      "        Security: societal       0.69      0.94      0.80        85\n",
      "                 Tradition       0.54      1.00      0.70        31\n",
      "         Conformity: rules       0.75      0.91      0.82        81\n",
      " Conformity: interpersonal       0.33      0.82      0.47        11\n",
      "                  Humility       0.37      1.00      0.54        13\n",
      "       Benevolence: caring       0.43      0.91      0.59        74\n",
      "Benevolence: dependability       0.34      0.71      0.46        42\n",
      "     Universalism: concern       0.66      0.94      0.77       125\n",
      "      Universalism: nature       0.50      0.88      0.64        17\n",
      "   Universalism: tolerance       0.25      0.65      0.36        34\n",
      " Universalism: objectivity       0.48      0.84      0.61        61\n",
      "\n",
      "                 micro avg       0.52      0.89      0.66       990\n",
      "                 macro avg       0.47      0.87      0.60       990\n",
      "              weighted avg       0.57      0.89      0.68       990\n",
      "               samples avg       0.56      0.91      0.66       990\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Threshold: {THRESHOLD}\")\n",
    "print(classification_report(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    target_names=LABEL_COLUMNS,\n",
    "    zero_division=0,\n",
    "))\n",
    "\n",
    "class_rep = classification_report(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    target_names=LABEL_COLUMNS,\n",
    "    zero_division=0,\n",
    "    output_dict=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Logging Optional\n",
    "neptune_logger.experiment[f\"threshold_selected_for_f1_custom_val_opt\"].log(THRESHOLD)\n",
    "\n",
    "for k in class_rep:\n",
    "    neptune_logger.experiment[f\"{k}_precision/Test\"].log(class_rep[k][\"precision\"])\n",
    "    neptune_logger.experiment[f\"{k}_recall/Test\"].log(class_rep[k][\"recall\"])\n",
    "    neptune_logger.experiment[f\"{k}_f1-score/Test\"].log(class_rep[k][\"f1-score\"])\n",
    "    neptune_logger.experiment[f\"{k}_support/Test\"].log(class_rep[k][\"support\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate f1-Score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6105788216966658\n"
     ]
    }
   ],
   "source": [
    "test_custom_f1 = -1\n",
    "test_macro_recall = class_rep[\"macro avg\"][\"recall\"]\n",
    "test_macro_precision = class_rep[\"macro avg\"][\"precision\"]\n",
    "if (test_macro_precision + test_macro_recall) != 0:\n",
    "    test_custom_f1 = (2*test_macro_recall*test_macro_precision/(test_macro_recall+test_macro_precision))\n",
    "else:\n",
    "    test_custom_f1 = 0\n",
    "print(test_custom_f1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# Optionally Log\n",
    "\n",
    "for i, name in enumerate(LABEL_COLUMNS):\n",
    "    auroc = AUROC(task=\"binary\")\n",
    "    class_roc_auc = auroc(predictions[:, i], labels[:, i])\n",
    "    # neptune_logger.experiment[f\"{name}_roc_auc/Test\"].log(class_roc_auc)\n",
    "\n",
    "auroc = AUROC(task=\"multilabel\", num_labels=len(LABEL_COLUMNS), average=\"micro\")\n",
    "total_auroc_micro = auroc(predictions, labels)\n",
    "\n",
    "auroc = AUROC(task=\"multilabel\", num_labels=len(LABEL_COLUMNS), average=\"macro\")\n",
    "total_auroc_macro = auroc(predictions, labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Log Metrics Optionally\n",
    "neptune_logger.experiment[f\"custom_f1/Test\"].log(test_custom_f1)\n",
    "neptune_logger.experiment[f\"roc_auc_total_macro/Test\"].log(total_auroc_macro)\n",
    "neptune_logger.experiment[f\"roc_auc_total_micro/Test\"].log(total_auroc_micro)\n",
    "neptune_logger.experiment.stop()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFvqzK4Ua-16"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Submission File\n",
    "Creating the submission file for one Model for the competition. (Note that the submitted systems are ensembles (ensemble_eval_and_predict.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_input = pd.read_csv('./data/arguments-test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "  Argument ID                          Conclusion       Stance  \\\n0      A26004    We should end affirmative action      against   \n1      A26010    We should end affirmative action  in favor of   \n2      A26016           We should ban naturopathy  in favor of   \n3      A26024  We should prohibit women in combat  in favor of   \n4      A26026           We should ban naturopathy  in favor of   \n\n                                             Premise  \\\n0   affirmative action helps with employment equity.   \n1  affirmative action can be considered discrimin...   \n2  naturopathy is very dangerous for the most vul...   \n3  women shouldn't be in combat because they aren...   \n4  once eradicated illnesses are returning due to...   \n\n                                                text  \n0  affirmative action helps with employment equit...  \n1  affirmative action can be considered discrimin...  \n2  naturopathy is very dangerous for the most vul...  \n3  women shouldn't be in combat because they aren...  \n4  once eradicated illnesses are returning due to...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Argument ID</th>\n      <th>Conclusion</th>\n      <th>Stance</th>\n      <th>Premise</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A26004</td>\n      <td>We should end affirmative action</td>\n      <td>against</td>\n      <td>affirmative action helps with employment equity.</td>\n      <td>affirmative action helps with employment equit...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A26010</td>\n      <td>We should end affirmative action</td>\n      <td>in favor of</td>\n      <td>affirmative action can be considered discrimin...</td>\n      <td>affirmative action can be considered discrimin...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A26016</td>\n      <td>We should ban naturopathy</td>\n      <td>in favor of</td>\n      <td>naturopathy is very dangerous for the most vul...</td>\n      <td>naturopathy is very dangerous for the most vul...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A26024</td>\n      <td>We should prohibit women in combat</td>\n      <td>in favor of</td>\n      <td>women shouldn't be in combat because they aren...</td>\n      <td>women shouldn't be in combat because they aren...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A26026</td>\n      <td>We should ban naturopathy</td>\n      <td>in favor of</td>\n      <td>once eradicated illnesses are returning due to...</td>\n      <td>once eradicated illnesses are returning due to...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_input[\"text\"] = test_df_input[\"Premise\"]+\" \" + test_df_input[\"Stance\"]+ \" \" + test_df_input[\"Conclusion\"]\n",
    "test_df_input.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1576 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7e34315862f040d18f94e305df230ef2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "trained_model = trained_model.to(device)\n",
    "\n",
    "test_df_dataset = BertDataset(\n",
    "    data=test_df_input,\n",
    "    tokenizer=TOKENIZER,\n",
    "    max_token_count=PARAMS[\"MAX_TOKEN_COUNT\"],\n",
    ")\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for item in tqdm(test_df_dataset):\n",
    "    _, prediction = trained_model(\n",
    "        item[\"input_ids\"].unsqueeze(dim=0).to(device),\n",
    "        item[\"attention_mask\"].unsqueeze(dim=0).to(device)\n",
    "    )\n",
    "    predictions.append(prediction.flatten())\n",
    "\n",
    "predictions = torch.stack(predictions).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predictions.numpy()\n",
    "upper, lower = 1, 0\n",
    "y_pred = np.where(y_pred > THRESHOLD, upper, lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Argument ID</th>\n",
       "      <th>Self-direction: thought</th>\n",
       "      <th>Self-direction: action</th>\n",
       "      <th>Stimulation</th>\n",
       "      <th>Hedonism</th>\n",
       "      <th>Achievement</th>\n",
       "      <th>Power: dominance</th>\n",
       "      <th>Power: resources</th>\n",
       "      <th>Face</th>\n",
       "      <th>Security: personal</th>\n",
       "      <th>...</th>\n",
       "      <th>Tradition</th>\n",
       "      <th>Conformity: rules</th>\n",
       "      <th>Conformity: interpersonal</th>\n",
       "      <th>Humility</th>\n",
       "      <th>Benevolence: caring</th>\n",
       "      <th>Benevolence: dependability</th>\n",
       "      <th>Universalism: concern</th>\n",
       "      <th>Universalism: nature</th>\n",
       "      <th>Universalism: tolerance</th>\n",
       "      <th>Universalism: objectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A26004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A26010</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A26016</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A26024</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A26026</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Argument ID  Self-direction: thought  Self-direction: action  Stimulation  \\\n",
       "0      A26004                        0                       0            0   \n",
       "1      A26010                        0                       0            0   \n",
       "2      A26016                        0                       0            0   \n",
       "3      A26024                        0                       0            0   \n",
       "4      A26026                        0                       0            0   \n",
       "\n",
       "   Hedonism  Achievement  Power: dominance  Power: resources  Face  \\\n",
       "0         0            1                 0                 0     0   \n",
       "1         0            0                 0                 0     0   \n",
       "2         0            1                 0                 0     0   \n",
       "3         0            1                 0                 0     0   \n",
       "4         0            0                 0                 0     0   \n",
       "\n",
       "   Security: personal  ...  Tradition  Conformity: rules  \\\n",
       "0                   1  ...          0                  0   \n",
       "1                   0  ...          0                  0   \n",
       "2                   1  ...          0                  0   \n",
       "3                   0  ...          0                  0   \n",
       "4                   1  ...          0                  0   \n",
       "\n",
       "   Conformity: interpersonal  Humility  Benevolence: caring  \\\n",
       "0                          0         0                    0   \n",
       "1                          0         0                    0   \n",
       "2                          0         0                    1   \n",
       "3                          0         0                    0   \n",
       "4                          0         0                    0   \n",
       "\n",
       "   Benevolence: dependability  Universalism: concern  Universalism: nature  \\\n",
       "0                           0                      1                     0   \n",
       "1                           0                      1                     0   \n",
       "2                           0                      1                     0   \n",
       "3                           0                      1                     0   \n",
       "4                           0                      0                     0   \n",
       "\n",
       "   Universalism: tolerance  Universalism: objectivity  \n",
       "0                        0                          0  \n",
       "1                        1                          0  \n",
       "2                        0                          1  \n",
       "3                        0                          0  \n",
       "4                        0                          1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_dictionary = {}\n",
    "prediction_dictionary[\"Argument ID\"] = test_df_input[\"Argument ID\"]\n",
    "for idx, l_name in enumerate(LABEL_COLUMNS):\n",
    "  prediction_dictionary[l_name]=y_pred[:,idx]\n",
    "\n",
    "test_prediction_df = pd.DataFrame(prediction_dictionary)\n",
    "test_prediction_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\dschr\\AppData\\Local\\Temp\\ipykernel_19996\\3780997615.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 1&gt;</span>              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\dschr\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_19996\\\\3780997615.py'</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'RUN_ID'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[31m╭─\u001B[0m\u001B[31m──────────────────────────────\u001B[0m\u001B[31m \u001B[0m\u001B[1;31mTraceback \u001B[0m\u001B[1;2;31m(most recent call last)\u001B[0m\u001B[31m \u001B[0m\u001B[31m───────────────────────────────\u001B[0m\u001B[31m─╮\u001B[0m\n",
       "\u001B[31m│\u001B[0m \u001B[33mC:\\Users\\dschr\\AppData\\Local\\Temp\\ipykernel_19996\\3780997615.py\u001B[0m:\u001B[94m1\u001B[0m in \u001B[92m<cell line: 1>\u001B[0m              \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m \u001B[3;31m[Errno 2] No such file or directory: \u001B[0m                                                            \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m \u001B[3;31m'C:\\\\Users\\\\dschr\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_19996\\\\3780997615.py'\u001B[0m                         \u001B[31m│\u001B[0m\n",
       "\u001B[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001B[0m\n",
       "\u001B[1;91mNameError: \u001B[0mname \u001B[32m'RUN_ID'\u001B[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if RUN_ID:\n",
    "    test_prediction_df.to_csv(f\"submissions/{RUN_ID}-submission_test.txt\", sep=\"\\t\", index=False)\n",
    "else:\n",
    "    test_prediction_df.to_csv(f\"submissions/{NAME}-submission_test.txt\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at single predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_example_prediction(record, show_all_probs=False, THRESHOLD=0.3):\n",
    "\n",
    "    print(record[\"Argument ID\"])\n",
    "    print(record[\"text\"])\n",
    "    print(f\"True Label: {record.category}\")\n",
    "\n",
    "\n",
    "    encoding = TOKENIZER.encode_plus(\n",
    "        record.text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,\n",
    "        return_token_type_ids=False,\n",
    "        padding=\"max_length\",\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    _, test_prediction = trained_model(encoding[\"input_ids\"], encoding[\"attention_mask\"])\n",
    "    test_prediction = test_prediction.flatten().numpy()\n",
    "\n",
    "    res = {}\n",
    "    if show_all_probs:\n",
    "        for label, prediction in zip(LABEL_COLUMNS, test_prediction):\n",
    "            print(f\"{label}: {prediction}\")\n",
    "            res[label] = prediction\n",
    "\n",
    "    else:\n",
    "        print(f\"Predictions:\")\n",
    "        for label, prediction in zip(LABEL_COLUMNS, test_prediction):\n",
    "            if prediction < THRESHOLD:\n",
    "                continue\n",
    "            print(f\"{label}: {prediction}\")\n",
    "            res[label] = prediction\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A18309\n",
      "social media gives it users a place to seek support when in need whether emotional or financially, things that would be more difficult if not impossible to do outside of their home. against Social media brings more harm than good\n",
      "True Label: ['Self-direction: action', 'Face', 'Security: personal', 'Benevolence: caring', 'Benevolence: dependability']\n",
      "Predictions:\n",
      "Self-direction: action: 0.49473991990089417\n",
      "Stimulation: 0.40371981263160706\n",
      "Hedonism: 0.4516661763191223\n",
      "Security: personal: 0.9821780323982239\n",
      "Benevolence: caring: 0.9349980354309082\n",
      "Universalism: tolerance: 0.327671617269516\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'Self-direction: action': 0.49473992,\n 'Stimulation': 0.4037198,\n 'Hedonism': 0.45166618,\n 'Security: personal': 0.98217803,\n 'Benevolence: caring': 0.93499804,\n 'Universalism: tolerance': 0.32767162}"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 13 whaling is good one\n",
    "trained_model.to(\"cpu\")\n",
    "test_record = test_df.iloc[6]\n",
    "print_example_prediction(test_record, show_all_probs=False, THRESHOLD=THRESHOLD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "14Ea4lIzsn5EFvPpYKtWStXEByT9qmbkj",
     "timestamp": 1667212470973
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
