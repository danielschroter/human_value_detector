{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Ensemble Evaluation, Optimal Threshold Selection and Prediction of Test Data\n",
    "In this Notebook we are going to determine the optimal decision threshold for an ensemble and then use the ensemble and optimal decision threshold to predict the test-set for the submission.\n",
    "\n",
    "Please Take a look at the README to setup the data and checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Install packages if on google colab\n",
    "!pip install -q pytorch-lightning==1.6.4 neptune-client transformers sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "COLAB = True\n",
    "\n",
    "\n",
    "pl.seed_everything(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "If you are training on google colab and want to connect to drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    import os\n",
    "    os.getcwd()\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cd ./drive/MyDrive/human_value/human_values_behind_arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Setup and Preprocessing\n",
    "We use Pytorch Lightning for the training and therefore import the Lighntning Data and Model Modules, as well as other helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from data_modules.BertDataModule import BertDataset\n",
    "from models.BertFineTunerPl import BertFineTunerPl\n",
    "from toolbox.bert_utils import max_for_thres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the Models that we want to ensemble. Download the Models used for the submission and place them in the checkpoint folder. Here you can then specify the path in to them in order to reproduce the results.  (If you want to ensemble different combinations just select them here. If you have own models trained then you can place them here too, but you need to ensure the params are loaded (see below))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PARAMS_ENSEMBLE = {\n",
    "    \"MODEL_CHECKPOINTS\": ['./checkpoints/HCV-409-microsoft-deberta-large-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-408-microsoft-deberta-large-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-406-microsoft-deberta-large-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-402-danschr-roberta-large-BS_16-EPOCHS_8-LR_5e-05-ACC_GRAD_2-MAX_LENGTH_165-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-403-danschr-roberta-large-BS_16-EPOCHS_8-LR_5e-05-ACC_GRAD_2-MAX_LENGTH_165-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-405-danschr-roberta-large-BS_16-EPOCHS_8-LR_5e-05-ACC_GRAD_2-MAX_LENGTH_165-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-364-microsoft-deberta-large-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-366-microsoft-deberta-large-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-368-microsoft-deberta-large-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-371-danschr-roberta-large-BS_16-EPOCHS_8-LR_5e-05-ACC_GRAD_2-MAX_LENGTH_165-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-372-danschr-roberta-large-BS_16-EPOCHS_8-LR_5e-05-ACC_GRAD_2-MAX_LENGTH_165-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-375-danschr-roberta-large-BS_16-EPOCHS_8-LR_5e-05-ACC_GRAD_2-MAX_LENGTH_165-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt'\n",
    "                          ],\n",
    "    \"DESCRIPTION\":\"FULL #3xDebL_F1 3EP 3xdanRobL_F1 3EP 3xDebL_Loss 3EP 3xdanRobL_Loss 3EP\",\n",
    "    \"TEST_PATH\" : \"./data/path_to_your_test_data.csv\",\n",
    "    \"LEAVE_OUT_DATA_PATH\": \"./data/leave_out_dataset_300.csv\",\n",
    "    \"MAX_THRESHOLD_METRIC\": \"custom\",\n",
    "    \"ENSEMBLE\": \"EN\",\n",
    "    \"LABEL_COLUMNS\":['Self-direction: thought',\n",
    "                     'Self-direction: action',\n",
    "                     'Stimulation',\n",
    "                     'Hedonism',\n",
    "                     'Achievement',\n",
    "                     'Power: dominance',\n",
    "                     'Power: resources',\n",
    "                     'Face',\n",
    "                     'Security: personal',\n",
    "                     'Security: societal',\n",
    "                     'Tradition',\n",
    "                     'Conformity: rules',\n",
    "                     'Conformity: interpersonal',\n",
    "                     'Humility',\n",
    "                     'Benevolence: caring',\n",
    "                     'Benevolence: dependability',\n",
    "                     'Universalism: concern',\n",
    "                     'Universalism: nature',\n",
    "                     'Universalism: tolerance',\n",
    "                     'Universalism: objectivity']\n",
    "}\n",
    "\n",
    "THRESHOLD = 0.26 # We compute it later on, but we set it's default value in case you want to skip this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We extract the model identifier e.g \"HCV-409\" from the checkpoint paths. (We use it later to pair the checkpoint together with the PARAMS ( Model Parameter used for training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HCV-3\n",
      "HCV-8\n",
      "HCV-11\n",
      "HCV-19\n",
      "HCV-2\n"
     ]
    }
   ],
   "source": [
    "# We extract the model identifier to log them and merge them with the corresponding parameter files\n",
    "NAME = \"\"\n",
    "ids = []\n",
    "for elem in PARAMS_ENSEMBLE[\"MODEL_CHECKPOINTS\"]:\n",
    "    text_list = elem.split(\"checkpoints/\")[1]\n",
    "    text_list = text_list.split(\"-\")\n",
    "    id = text_list[0]+\"-\" + text_list[1]\n",
    "    ids.append(id)\n",
    "    NAME= NAME + \"_\" + id\n",
    "    print(text_list[0]+\"-\" + text_list[1])\n",
    "NAME = PARAMS_ENSEMBLE[\"ENSEMBLE\"]+\"_\"+NAME[1:]\n",
    "\n",
    "PARAMS_ENSEMBLE[\"IDS\"] = ids\n",
    "LABEL_COLUMNS = PARAMS_ENSEMBLE[\"LABEL_COLUMNS\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## The Ensemble List\n",
    "\n",
    "We create a dictionary containing model's checkoint path, Model identifier and Parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loading the parameters for each model\n",
    "PARAMS_LIST = []\n",
    "for id in PARAMS_ENSEMBLE[\"IDS\"]:\n",
    "    with open(f'./checkpoints/{id}_PARAMS.pkl', 'rb') as f:\n",
    "        loaded_dict = pickle.load(f)\n",
    "        PARAMS_LIST.append(loaded_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We group together the checkpoint and parameters in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Concatenating relevant information into one Ensemble_list: Parameters, Id, and Path to Checkpoint.\n",
    "ENSEMBLE_LIST = []\n",
    "for param, id, mc in zip(PARAMS_LIST, PARAMS_ENSEMBLE[\"IDS\"], PARAMS_ENSEMBLE[\"MODEL_CHECKPOINTS\"]):\n",
    "    ENSEMBLE_LIST.append({\"PARAMS\":param, \"ID\":id,\"MODEL_CHECKPOINT\":mc})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Prediction of Leave-Out-Dataset\n",
    "\n",
    "We compute the optimal decision threshold on the Leave-Out-Dataset. Section can be skipped. Optimal THRESHOLD is 0.26. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "leave_out_data = pd.read_csv(PARAMS_ENSEMBLE[\"LEAVE_OUT_DATA_PATH\"], index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_unseen_data(trained_model, data, collect_labels=True):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    trained_model = trained_model.to(device)\n",
    "\n",
    "    test_dataset = BertDataset(\n",
    "        data=data,\n",
    "        tokenizer=TOKENIZER,\n",
    "        max_token_count=PARAMS[\"MAX_TOKEN_COUNT\"],\n",
    "    )\n",
    "\n",
    "    predictions = []\n",
    "    labels =[]\n",
    "\n",
    "    for item in tqdm(test_dataset):\n",
    "        _, prediction = trained_model(\n",
    "            item[\"input_ids\"].unsqueeze(dim=0).to(device),\n",
    "            item[\"attention_mask\"].unsqueeze(dim=0).to(device)\n",
    "        )\n",
    "        predictions.append(prediction.flatten())\n",
    "        if collect_labels:\n",
    "            labels.append(item[\"labels\"].int())\n",
    "\n",
    "    predictions = torch.stack(predictions).detach().cpu()\n",
    "    if collect_labels:\n",
    "        labels = torch.stack(predictions).detach().cpu()\n",
    "\n",
    "    return predictions, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We iterate over the Models in the Ensemble List and get the predictions for the leave-out-dataset and for the test-dataset for each model (If we use a test-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Iterate over elements in Ensemble_List and get predictions from each model. Collect them in predictions [] list.\n",
    "predictions = []\n",
    "labels = []\n",
    "for idx, elem in enumerate(ENSEMBLE_LIST):\n",
    "    print(f\"Starting with model {elem['MODEL_CHECKPOINT']}\")\n",
    "    PARAMS = elem[\"PARAMS\"]\n",
    "    trained_model = BertFineTunerPl.load_from_checkpoint(\n",
    "        elem[\"MODEL_CHECKPOINT\"],\n",
    "        params=PARAMS,\n",
    "        label_columns=LABEL_COLUMNS,\n",
    "        n_classes=len(LABEL_COLUMNS)\n",
    "    )\n",
    "    trained_model.eval()\n",
    "    trained_model.freeze()\n",
    "    print(f\"With Tokenizer {PARAMS['MODEL_PATH']}\")\n",
    "    TOKENIZER = AutoTokenizer.from_pretrained(PARAMS[\"MODEL_PATH\"])\n",
    "    pred, lab = predict_unseen_data(trained_model=trained_model, data=leave_out_data)\n",
    "    predictions.append(pred)\n",
    "    labels.append(lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Predictions\n",
    "\n",
    "Average the predictions to ensemble the different opinions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels_val = labels[0]\n",
    "\n",
    "y_pred_val = torch.stack(predictions).numpy()\n",
    "y_true_val = labels_val.numpy()\n",
    "\n",
    "y_pred_val_avg = np.mean(y_pred_val, axis=0)\n",
    "\n",
    "y_pred_val_avg_tensor = torch.tensor(y_pred_val_avg)\n",
    "y_true_val_tensor =torch.tensor(y_true_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Threshold\n",
    "\n",
    "Determin the optimal threshold on the leave out dataset. Use this threshold later for the final prediction on the test-data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "THRESHOLD = max_for_thres(y_pred=y_pred_val_avg_tensor, y_true=y_true_val_tensor, label_columns=LABEL_COLUMNS, average=PARAMS_ENSEMBLE[\"MAX_THRESHOLD_METRIC\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Predicting the submission File.\n",
    "Now that we have the optimal threshold, we can create the submission file. (Note that this is the same code as in predict.ipynb. But we will show below. But we will show below how we further used stacking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_df_input = pd.read_csv('./data/arguments-test.tsv', sep='\\t')\n",
    "test_df_input[\"text\"] = test_df_input[\"Premise\"]+\" \" + test_df_input[\"Stance\"]+ \" \" + test_df_input[\"Conclusion\"]\n",
    "test_df_input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate predictions for all Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions_test = []\n",
    "for idx, elem in enumerate(ENSEMBLE_LIST):\n",
    "    print(f\"Starting with model {elem['MODEL_CHECKPOINT']}\")\n",
    "    PARAMS = elem[\"PARAMS\"]\n",
    "    trained_model = BertFineTunerPl.load_from_checkpoint(\n",
    "        elem[\"MODEL_CHECKPOINT\"],\n",
    "        params=PARAMS,\n",
    "        label_columns=LABEL_COLUMNS,\n",
    "        n_classes=len(LABEL_COLUMNS)\n",
    "    )\n",
    "    trained_model.eval()\n",
    "    trained_model.freeze()\n",
    "    print(f\"With Tokenizer {PARAMS['MODEL_PATH']}\")\n",
    "    TOKENIZER = AutoTokenizer.from_pretrained(PARAMS[\"MODEL_PATH\"])\n",
    "\n",
    "    pred, lab = predict_unseen_data(trained_model=trained_model, data=test_df_input, collect_labels=False)\n",
    "    predictions_test.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions_test_stacked = torch.stack(predictions_test).numpy()\n",
    "predictions_avg = np.mean(predictions_test_stacked, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binarize predictions with previously computed threshold to derive final labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "upper, lower = 1, 0\n",
    "y_pred = np.where(predictions_avg > THRESHOLD, upper, lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction_dictionary = {}\n",
    "prediction_dictionary[\"Argument ID\"] = test_df_input[\"Argument ID\"]\n",
    "for idx, l_name in enumerate(LABEL_COLUMNS):\n",
    "    prediction_dictionary[l_name]=y_pred[:,idx]\n",
    "\n",
    "test_prediction_df = pd.DataFrame(prediction_dictionary)\n",
    "test_prediction_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_prediction_df.to_csv(f\"submissions/submission_test.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how we created the final submission for the best performing system. \n",
    "In the subsequent section we apply stacking and create the variations of the system that were also submitted for the competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Stacking (optional)\n",
    "In the following we train logistic regressions to determine the decision threshold for each label. We train the model on the training-dataset. So we get the predictions for the training dataset and train the models in a way that they schould learn to predict the labels based on the predictions as input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./data/data_training_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "labels = []\n",
    "for idx, elem in enumerate(ENSEMBLE_LIST):\n",
    "    print(f\"Starting with model {elem['MODEL_CHECKPOINT']}\")\n",
    "    PARAMS = elem[\"PARAMS\"]\n",
    "    trained_model = BertFineTunerPl.load_from_checkpoint(\n",
    "        elem[\"MODEL_CHECKPOINT\"],\n",
    "        params=PARAMS,\n",
    "        label_columns=LABEL_COLUMNS,\n",
    "        n_classes=len(LABEL_COLUMNS)\n",
    "    )\n",
    "    trained_model.eval()\n",
    "    trained_model.freeze()\n",
    "    print(f\"With Tokenizer {PARAMS['MODEL_PATH']}\")\n",
    "    TOKENIZER = AutoTokenizer.from_pretrained(PARAMS[\"MODEL_PATH\"])\n",
    "\n",
    "    pred, lab = predict_unseen_data(trained_model=trained_model, data=train_df, collect_labels=True)\n",
    "    predictions.append(pred)\n",
    "    labels.append(lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Train Logistic Regression\n",
    "We structure our input-data and then train the logistic regressions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For each sample in the data we concatenate the prediction of each model columnwise. So we get a the shape [len(data), 20*num_models_in_ensemble]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels_val = labels[0]\n",
    "predictions_val = torch.Tensor([])\n",
    "for p in predictions:\n",
    "    predictions_val = torch.cat([predictions_val, p], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "logReg = MultiOutputClassifier(LogisticRegression(random_state=0, max_iter=200))\n",
    "# logReg=MultiOutputClassifier(MultinomialNB(alpha=0.1))\n",
    "# logReg=MultiOutputClassifier(DecisionTreeClassifier(min_samples_leaf=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logReg.fit(predictions_val.numpy(), labels_val.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Get the unstacked predictions for the test-file from above and concatenate the predictions from each model columnwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions_transformed = torch.Tensor([])\n",
    "for record in predictions_test_stacked:\n",
    "    predictions_transformed = torch.cat([predictions_test, record], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Use the trained logReg Model to predict the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = logReg.predict(predictions_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction_dictionary = {}\n",
    "prediction_dictionary[\"Argument ID\"] = test_df_input[\"Argument ID\"]\n",
    "for idx, l_name in enumerate(LABEL_COLUMNS):\n",
    "    prediction_dictionary[l_name]=y_pred[:,idx]\n",
    "\n",
    "test_prediction_df = pd.DataFrame(prediction_dictionary)\n",
    "test_prediction_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_prediction_df.to_csv(f\"submissions/test-submission_logReg\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
