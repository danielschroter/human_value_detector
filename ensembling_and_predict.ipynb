{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Ensemble Evaluation, Optimal Threshold Selection and Prediction of Test Data\n",
    "In this Notebook we are going to determine the optimal decision threshold for an ensemble and then use the ensemble and optimal decision threshold to predict the test-set for the submission."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Install packages if on google colab\n",
    "!pip install -q pytorch-lightning==1.6.4 neptune-client transformers sentencepiece"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": "42"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import NeptuneLogger\n",
    "\n",
    "from torchmetrics import  AUROC\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "COLAB = True\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "\n",
    "\n",
    "pl.seed_everything(RANDOM_SEED)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you are training to google colab and want to connect to drive"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    import os\n",
    "    os.getcwd()\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cd ./drive/MyDrive/human_value/human_values_behind_arguments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!git pull"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Modules\n",
    "We use Pytorch Lightning for the training and therefore import the Lighntning Data and Model Modules, as well as other helper functions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "from data_modules.BertDataModule import BertDataModule, BertDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from models.BertFineTunerPl import BertFineTunerPl\n",
    "from toolbox.bert_utils import max_for_thres"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "PARAMS_ENSEMBLE = {\n",
    "    \"MODEL_CHECKPOINTS\": ['./checkpoints/HCV-409-microsoft-deberta-large-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-408-microsoft-deberta-large-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-406-microsoft-deberta-large-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-402-danschr-roberta-large-BS_16-EPOCHS_8-LR_5e-05-ACC_GRAD_2-MAX_LENGTH_165-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-403-danschr-roberta-large-BS_16-EPOCHS_8-LR_5e-05-ACC_GRAD_2-MAX_LENGTH_165-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-405-danschr-roberta-large-BS_16-EPOCHS_8-LR_5e-05-ACC_GRAD_2-MAX_LENGTH_165-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-364-microsoft-deberta-large-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-366-microsoft-deberta-large-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-368-microsoft-deberta-large-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-371-danschr-roberta-large-BS_16-EPOCHS_8-LR_5e-05-ACC_GRAD_2-MAX_LENGTH_165-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-372-danschr-roberta-large-BS_16-EPOCHS_8-LR_5e-05-ACC_GRAD_2-MAX_LENGTH_165-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-375-danschr-roberta-large-BS_16-EPOCHS_8-LR_5e-05-ACC_GRAD_2-MAX_LENGTH_165-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt'\n",
    "                          ],\n",
    "    \"DESCRIPTION\":\"FULL #3xDebL_F1 3EP 3xdanRobL_F1 3EP 3xDebL_Loss 3EP 3xdanRobL_Loss 3EP\",\n",
    "    \"TEST_PATH\" : \"./data/path_to_your_test_data.csv\",\n",
    "    \"LEAVE_OUT_DATA_PATH\": \"./data/leave_out_dataset_300.csv\",\n",
    "    \"MAX_THRESHOLD_METRIC\": \"custom\",\n",
    "    \"ENSEMBLE\": \"EN\",\n",
    "    \"LABEL_COLUMNS\":['Self-direction: thought',\n",
    "                     'Self-direction: action',\n",
    "                     'Stimulation',\n",
    "                     'Hedonism',\n",
    "                     'Achievement',\n",
    "                     'Power: dominance',\n",
    "                     'Power: resources',\n",
    "                     'Face',\n",
    "                     'Security: personal',\n",
    "                     'Security: societal',\n",
    "                     'Tradition',\n",
    "                     'Conformity: rules',\n",
    "                     'Conformity: interpersonal',\n",
    "                     'Humility',\n",
    "                     'Benevolence: caring',\n",
    "                     'Benevolence: dependability',\n",
    "                     'Universalism: concern',\n",
    "                     'Universalism: nature',\n",
    "                     'Universalism: tolerance',\n",
    "                     'Universalism: objectivity']\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We extract the identifier e.g \"HCV-409\" from the checkpoint paths. (We use it later to pair the checkpoint together with the PARAMS ( Model Parameter used for training)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HCV-3\n",
      "HCV-8\n",
      "HCV-11\n",
      "HCV-19\n",
      "HCV-2\n"
     ]
    }
   ],
   "source": [
    "NAME = \"\"\n",
    "ids = []\n",
    "for elem in PARAMS_ENSEMBLE[\"MODEL_CHECKPOINTS\"]:\n",
    "    text_list = elem.split(\"checkpoints/\")[1]\n",
    "    text_list = text_list.split(\"-\")\n",
    "    id = text_list[0]+\"-\" + text_list[1]\n",
    "    ids.append(id)\n",
    "    NAME= NAME + \"_\" + id\n",
    "    print(text_list[0]+\"-\" + text_list[1])\n",
    "NAME = PARAMS_ENSEMBLE[\"ENSEMBLE\"]+\"_\"+NAME[1:]\n",
    "\n",
    "PARAMS_ENSEMBLE[\"IDS\"] = ids\n",
    "LABEL_COLUMNS = PARAMS_ENSEMBLE[\"LABEL_COLUMNS\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Data\n",
    "Load the Leave out Dataset to determine the optimal threshold"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "val_df = pd.read_csv(PARAMS_ENSEMBLE[\"LEAVE_OUT_DATA_PATH\"], index_col=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Ensemble List"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Take IDs that have been generated and get the params file with the same id\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Loading the parameters for each model\n",
    "PARAMS_LIST = []\n",
    "for id in PARAMS_ENSEMBLE[\"IDS\"]:\n",
    "    with open(f'./checkpoints/{id}_PARAMS.pkl', 'rb') as f:\n",
    "        loaded_dict = pickle.load(f)\n",
    "        PARAMS_LIST.append(loaded_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We group together the checkpoint and parameters in a list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Concatenating relevant information into one Ensemble_list: Parameters, Id, and Path to Checkpoint.\n",
    "ENSEMBLE_LIST = []\n",
    "for param, id, mc in zip(PARAMS_LIST, PARAMS_ENSEMBLE[\"IDS\"], PARAMS_ENSEMBLE[\"MODEL_CHECKPOINTS\"]):\n",
    "    ENSEMBLE_LIST.append({\"PARAMS\":param, \"ID\":id,\"MODEL_CHECKPOINT\":mc})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prediction of Leave-Out-Dataset\n",
    "The following sections contain a lot of logging to neptune. If you wish to track the different performance with neptune you can uncomment the corresponding lines..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict_unseen_data(trained_model, data, collect_labels=True):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    trained_model = trained_model.to(device)\n",
    "\n",
    "    test_dataset = BertDataset(\n",
    "        data=data,\n",
    "        tokenizer=TOKENIZER,\n",
    "        max_token_count=PARAMS[\"MAX_TOKEN_COUNT\"],\n",
    "    )\n",
    "\n",
    "    predictions = []\n",
    "    labels =[]\n",
    "\n",
    "    for item in tqdm(test_dataset):\n",
    "        _, prediction = trained_model(\n",
    "            item[\"input_ids\"].unsqueeze(dim=0).to(device),\n",
    "            item[\"attention_mask\"].unsqueeze(dim=0).to(device)\n",
    "        )\n",
    "        predictions.append(prediction.flatten())\n",
    "        if collect_labels:\n",
    "            labels.append(item[\"labels\"].int())\n",
    "\n",
    "    predictions = torch.stack(predictions).detach().cpu()\n",
    "    if collect_labels:\n",
    "        labels = torch.stack(predictions).detach().cpu()\n",
    "\n",
    "    return predictions, labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We iterate over the Models in the Ensemble List and get the predictions for the leave-out-dataset and for the test-dataset for each model (If we use a test-dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over elements in Ensemble_List and get predictions from each model. Collect them in predictions [] list.\n",
    "predictions = []\n",
    "labels = []\n",
    "for idx, elem in enumerate(ENSEMBLE_LIST):\n",
    "    print(f\"Starting with model {elem['MODEL_CHECKPOINT']}\")\n",
    "    PARAMS = elem[\"PARAMS\"]\n",
    "    trained_model = BertFineTunerPl.load_from_checkpoint(\n",
    "        elem[\"MODEL_CHECKPOINT\"],\n",
    "        params=PARAMS,\n",
    "        label_columns=LABEL_COLUMNS,\n",
    "        n_classes=len(LABEL_COLUMNS)\n",
    "    )\n",
    "    trained_model.eval()\n",
    "    trained_model.freeze()\n",
    "    print(f\"With Tokenizer {PARAMS['MODEL_PATH']}\")\n",
    "    TOKENIZER = AutoTokenizer.from_pretrained(PARAMS[\"MODEL_PATH\"])\n",
    "    pred, lab = predict_unseen_data(trained_model=trained_model, data=val_df)\n",
    "    predictions.append(pred)\n",
    "    labels.append(lab)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels_val = labels[0]\n",
    "\n",
    "y_pred_val = torch.stack(predictions).numpy()\n",
    "y_true_val = labels_val.numpy()\n",
    "\n",
    "y_pred_val_avg = np.mean(y_pred_val, axis=0)\n",
    "\n",
    "y_pred_val_avg_tensor = torch.tensor(y_pred_val_avg)\n",
    "y_true_val_tensor =torch.tensor(y_true_val)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "THRESHOLD = max_for_thres(y_pred=y_pred_val_avg_tensor, y_true=y_true_val_tensor, label_columns=LABEL_COLUMNS, average=PARAMS_ENSEMBLE[\"MAX_THRESHOLD_METRIC\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predicting the submission File.\n",
    "Now that we have the optimal threshold, we can create the submission file. (Note that this is the same code as in predict.ipynb. But we will show below. But we will show below how we further used stacking."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_df_input = pd.read_csv('./data/arguments-test.tsv', sep='\\t')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_df_input[\"text\"] = test_df_input[\"Premise\"]+\" \" + test_df_input[\"Stance\"]+ \" \" + test_df_input[\"Conclusion\"]\n",
    "test_df_input.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions_test = []\n",
    "for idx, elem in enumerate(ENSEMBLE_LIST):\n",
    "    print(f\"Starting with model {elem['MODEL_CHECKPOINT']}\")\n",
    "    PARAMS = elem[\"PARAMS\"]\n",
    "    trained_model = BertFineTunerPl.load_from_checkpoint(\n",
    "        elem[\"MODEL_CHECKPOINT\"],\n",
    "        params=PARAMS,\n",
    "        label_columns=LABEL_COLUMNS,\n",
    "        n_classes=len(LABEL_COLUMNS)\n",
    "    )\n",
    "    trained_model.eval()\n",
    "    trained_model.freeze()\n",
    "    print(f\"With Tokenizer {PARAMS['MODEL_PATH']}\")\n",
    "    TOKENIZER = AutoTokenizer.from_pretrained(PARAMS[\"MODEL_PATH\"])\n",
    "\n",
    "    pred, lab = predict_unseen_data(trained_model=trained_model, data=test_df_input, collect_labels=False)\n",
    "    predictions_test.append(pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions_test_stacked = torch.stack(predictions_test).numpy()\n",
    "predictions_avg = np.mean(predictions_test_stacked, axis=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "upper, lower = 1, 0\n",
    "y_pred = np.where(predictions_avg > THRESHOLD, upper, lower)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prediction_dictionary = {}\n",
    "prediction_dictionary[\"Argument ID\"] = test_df_input[\"Argument ID\"]\n",
    "for idx, l_name in enumerate(LABEL_COLUMNS):\n",
    "    prediction_dictionary[l_name]=y_pred[:,idx]\n",
    "\n",
    "test_prediction_df = pd.DataFrame(prediction_dictionary)\n",
    "test_prediction_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_prediction_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [4]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtest_prediction_df\u001B[49m\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msubmissions/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mRUN_ID\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m-submission_validation.tsv\u001B[39m\u001B[38;5;124m\"\u001B[39m, sep\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'test_prediction_df' is not defined"
     ]
    }
   ],
   "source": [
    "test_prediction_df.to_csv(f\"submissions/submission_test.tsv\", sep=\"\\t\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Stacking\n",
    "In the following we train logistic regressions to determine the decision threshold for each label. We train the model on the training-dataset. So we get the predictions for the training dataset and train the models in a way that they schould learn to predict the labels based on the predictions as input.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./data/data_training_full.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions = []\n",
    "labels = []\n",
    "for idx, elem in enumerate(ENSEMBLE_LIST):\n",
    "    print(f\"Starting with model {elem['MODEL_CHECKPOINT']}\")\n",
    "    PARAMS = elem[\"PARAMS\"]\n",
    "    trained_model = BertFineTunerPl.load_from_checkpoint(\n",
    "        elem[\"MODEL_CHECKPOINT\"],\n",
    "        params=PARAMS,\n",
    "        label_columns=LABEL_COLUMNS,\n",
    "        n_classes=len(LABEL_COLUMNS)\n",
    "    )\n",
    "    trained_model.eval()\n",
    "    trained_model.freeze()\n",
    "    print(f\"With Tokenizer {PARAMS['MODEL_PATH']}\")\n",
    "    TOKENIZER = AutoTokenizer.from_pretrained(PARAMS[\"MODEL_PATH\"])\n",
    "\n",
    "    pred, lab = predict_unseen_data(trained_model=trained_model, data=train_df, collect_labels=True)\n",
    "    predictions.append(pred)\n",
    "    labels.append(lab)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train Logistic Regression\n",
    "We structure our input-data and then train the logistic regressions.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For each sample in the data we concatenate the prediction of each model columnwise. So we get a the shape [len(data), 20*num_models_in_ensemble]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels_val = labels[0]\n",
    "predictions_val = torch.Tensor([])\n",
    "for p in predictions:\n",
    "    predictions_val = torch.cat([predictions_val, p], dim=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "logReg = MultiOutputClassifier(LogisticRegression(random_state=0, max_iter=200))\n",
    "# logReg=MultiOutputClassifier(MultinomialNB(alpha=0.1))\n",
    "# logReg=MultiOutputClassifier(DecisionTreeClassifier(min_samples_leaf=3))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "logReg.fit(predictions_val.numpy(), labels_val.numpy())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get the unstacked predictions for the test-file from above and concatenate the predictions from each model columnwise."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions_transformed = torch.Tensor([])\n",
    "for record in predictions_test_stacked:\n",
    "    predictions_transformed = torch.cat([predictions_test, record], dim=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use the trained logReg Model to predict the labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred = logReg.predict(predictions_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create Submission File"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prediction_dictionary = {}\n",
    "prediction_dictionary[\"Argument ID\"] = test_df_input[\"Argument ID\"]\n",
    "for idx, l_name in enumerate(LABEL_COLUMNS):\n",
    "    prediction_dictionary[l_name]=y_pred[:,idx]\n",
    "\n",
    "test_prediction_df = pd.DataFrame(prediction_dictionary)\n",
    "test_prediction_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_prediction_df.to_csv(f\"submissions/test-submission_logReg\", sep=\"\\t\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
