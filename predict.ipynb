{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install -q pytorch-lightning==1.6.4 neptune-client transformers sentencepiece"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import Modules"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": "42"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import pickle\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "COLAB = True\n",
    "\n",
    "pl.seed_everything(RANDOM_SEED)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Connect to GoogleDrive if running on Colab"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    import os\n",
    "    os.getcwd()\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cd ./drive/MyDrive/human_value/human_value_detector"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!git pull"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Pytorch-Lightning Model and set paths to model-checkpoints.\n",
    "These Models are ensembled together to make the prediction on the test-file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "\u001B[31m╭─\u001B[0m\u001B[31m──────────────────────────────\u001B[0m\u001B[31m \u001B[0m\u001B[1;31mTraceback \u001B[0m\u001B[1;2;31m(most recent call last)\u001B[0m\u001B[31m \u001B[0m\u001B[31m───────────────────────────────\u001B[0m\u001B[31m─╮\u001B[0m\n\u001B[31m│\u001B[0m \u001B[33mC:\\Users\\dschr\\AppData\\Local\\Temp\\ipykernel_11956\\3507523594.py\u001B[0m:\u001B[94m2\u001B[0m in \u001B[92m<cell line: 2>\u001B[0m              \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[3;31m[Errno 2] No such file or directory: \u001B[0m                                                            \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[3;31m'C:\\\\Users\\\\dschr\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_11956\\\\3507523594.py'\u001B[0m                         \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[33mC:\\Users\\dschr\\DataspellProjects\\human_value_detector\\models\\BertFineTunerPl.py\u001B[0m:\u001B[94m18\u001B[0m in \u001B[92m<module>\u001B[0m   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 15 \u001B[0m\u001B[94mfrom\u001B[0m \u001B[4;96mtorch\u001B[0m\u001B[4;96m.\u001B[0m\u001B[4;96moptim\u001B[0m \u001B[94mimport\u001B[0m AdamW                                                              \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 16 \u001B[0m\u001B[94mfrom\u001B[0m \u001B[4;96msklearn\u001B[0m\u001B[4;96m.\u001B[0m\u001B[4;96mmetrics\u001B[0m \u001B[94mimport\u001B[0m classification_report                                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 17 \u001B[0m                                                                                           \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m 18 \u001B[94mfrom\u001B[0m \u001B[4;96mtoolbox\u001B[0m\u001B[4;96m.\u001B[0m\u001B[4;96mbert_utils\u001B[0m \u001B[94mimport\u001B[0m max_for_thres                                               \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 19 \u001B[0m                                                                                           \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 20 \u001B[0m\u001B[94mclass\u001B[0m \u001B[4;92mBertFineTunerPl\u001B[0m(pl.LightningModule):                                                 \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 21 \u001B[0m                                                                                           \u001B[31m│\u001B[0m\n\u001B[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001B[0m\n\u001B[1;91mModuleNotFoundError: \u001B[0mNo module named \u001B[32m'toolbox'\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\dschr\\AppData\\Local\\Temp\\ipykernel_11956\\3507523594.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 2&gt;</span>              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\dschr\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_11956\\\\3507523594.py'</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\dschr\\DataspellProjects\\human_value_detector\\models\\BertFineTunerPl.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">18</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 15 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">torch.optim</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> AdamW                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 16 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">sklearn.metrics</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> classification_report                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 17 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 18 <span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">toolbox.bert_utils</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> max_for_thres                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 19 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 20 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">class</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; text-decoration: underline\">BertFineTunerPl</span>(pl.LightningModule):                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 21 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ModuleNotFoundError: </span>No module named <span style=\"color: #008000; text-decoration-color: #008000\">'toolbox'</span>\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from data_modules.BertDataModule import BertDataset\n",
    "from models.BertFineTunerPl import BertFineTunerPl"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "PARAMS_ENSEMBLE = {\n",
    "    \"MODEL_CHECKPOINTS\": ['./checkpoints/HCV-409-microsoft-deberta-large-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-408-microsoft-deberta-large-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-406-microsoft-deberta-large-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-402-danschr-roberta-large-BS_16-EPOCHS_8-LR_5e-05-ACC_GRAD_2-MAX_LENGTH_165-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-403-danschr-roberta-large-BS_16-EPOCHS_8-LR_5e-05-ACC_GRAD_2-MAX_LENGTH_165-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-405-danschr-roberta-large-BS_16-EPOCHS_8-LR_5e-05-ACC_GRAD_2-MAX_LENGTH_165-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-364-microsoft-deberta-large-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-366-microsoft-deberta-large-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-368-microsoft-deberta-large-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-371-danschr-roberta-large-BS_16-EPOCHS_8-LR_5e-05-ACC_GRAD_2-MAX_LENGTH_165-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-372-danschr-roberta-large-BS_16-EPOCHS_8-LR_5e-05-ACC_GRAD_2-MAX_LENGTH_165-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-375-danschr-roberta-large-BS_16-EPOCHS_8-LR_5e-05-ACC_GRAD_2-MAX_LENGTH_165-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt'\n",
    "                          ],\n",
    "    \"DESCRIPTION\":\"FULL #3xDebL_F1 3EP 3xdanRobL_F1 3EP 3xDebL_Loss 3EP 3xdanRobL_Loss 3EP\",\n",
    "    \"TEST_PATH\" : \"./data/arguments-test.tsv\",\n",
    "    \"MAX_THRESHOLD_METRIC\": \"custom\",\n",
    "    \"ENSEMBLE\": \"EN\",\n",
    "    \"ENSEMBLE_THRESHOLD\":0.26,\n",
    "    \"LABEL_COLUMNS\":['Self-direction: thought',\n",
    "                     'Self-direction: action',\n",
    "                     'Stimulation',\n",
    "                     'Hedonism',\n",
    "                     'Achievement',\n",
    "                     'Power: dominance',\n",
    "                     'Power: resources',\n",
    "                     'Face',\n",
    "                     'Security: personal',\n",
    "                     'Security: societal',\n",
    "                     'Tradition',\n",
    "                     'Conformity: rules',\n",
    "                     'Conformity: interpersonal',\n",
    "                     'Humility',\n",
    "                     'Benevolence: caring',\n",
    "                     'Benevolence: dependability',\n",
    "                     'Universalism: concern',\n",
    "                     'Universalism: nature',\n",
    "                     'Universalism: tolerance',\n",
    "                     'Universalism: objectivity']\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the Identifier within Model-Path. (Used to get corresponding PARAMS File)\n",
    "NAME = \"\"\n",
    "ids = []\n",
    "for elem in PARAMS_ENSEMBLE[\"MODEL_CHECKPOINTS\"]:\n",
    "    text_list = elem.split(\"checkpoints/\")[1]\n",
    "    text_list = text_list.split(\"-\")\n",
    "    id = text_list[0]+\"-\" + text_list[1]\n",
    "    ids.append(id)\n",
    "    NAME= NAME + \"_\" + id\n",
    "    print(text_list[0]+\"-\" + text_list[1])\n",
    "NAME = PARAMS_ENSEMBLE[\"ENSEMBLE\"]+\"_\"+NAME[1:]\n",
    "\n",
    "PARAMS_ENSEMBLE[\"IDS\"] = ids\n",
    "LABEL_COLUMNS = PARAMS_ENSEMBLE[\"LABEL_COLUMNS\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Defining the Model PARAMS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Take IDs that have been generated and get the params_file with the same id"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Loading the parameters for each model\n",
    "PARAMS_LIST = []\n",
    "for id in PARAMS_ENSEMBLE[\"IDS\"]:\n",
    "    with open(f'./checkpoints/{id}_PARAMS.pkl', 'rb') as f:\n",
    "        loaded_dict = pickle.load(f)\n",
    "        PARAMS_LIST.append(loaded_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'PARAMS': {'MODEL_PATH': 'danschr/roberta-large-BS_16-EPOCHS_8-LR_5e-05-ACC_GRAD_2-MAX_LENGTH_165',\n   'BATCH_SIZE': 8,\n   'LR': 2e-05,\n   'EPOCHS': 20,\n   'DEVICE': device(type='cuda'),\n   'NUM_TRAIN_WORKERS': 4,\n   'NUM_VAL_WORKERS': 4,\n   'MAX_TOKEN_COUNT': 165,\n   'SEED': 42,\n   'WEIGHTS': [0.54238067,\n    0.38692124,\n    2.28156376,\n    3.17629465,\n    0.36239603,\n    0.89006059,\n    0.87405231,\n    1.44634846,\n    0.27533886,\n    0.31700788,\n    0.97389395,\n    0.46999331,\n    2.6127585,\n    1.34618582,\n    0.41359411,\n    0.70227324,\n    0.26240447,\n    1.32057903,\n    0.82508163,\n    0.52087147],\n   'WEIGHTS_APPLIED': False,\n   'CRITERION': [BCEWithLogitsLoss()],\n   'DATA': 'Premise + Stance + Conclusion',\n   'OPTIMIZER': 'AdamW',\n   'ACCUMULATE_GRAD_BATCHES': 1,\n   'PATIENCE': 3,\n   'VAL_CHECK_INTERVAL': 300,\n   'MAX_THRESHOLD_METRIC': 'custom',\n   'EARLY_STOPPING_METRIC': 'avg_val_loss',\n   'EARLY_STOPPING_MODE': 'min',\n   'ACTIVATION': None,\n   'DROPOUT': None,\n   'HIDDEN_LAYERS': None,\n   'TRAIN_PATH': 'data_training_individual_v2_300_500.csv',\n   'VALIDATION_PATH': None,\n   'TEST_PATH': 'data_test_individual_v2_500.csv'},\n  'ID': 'HCV-15',\n  'MODEL_CHECKPOINT': './checkpoints/HCV-15-roberta-base-BS_8-LR_2e-05-HL_None-DROPOUT_None.ckpt'},\n {'PARAMS': {'MODEL_PATH': 'danschr/roberta-large-BS_16-EPOCHS_8-LR_5e-05-ACC_GRAD_2-MAX_LENGTH_165',\n   'BATCH_SIZE': 8,\n   'LR': 2e-05,\n   'EPOCHS': 20,\n   'DEVICE': device(type='cuda'),\n   'NUM_TRAIN_WORKERS': 4,\n   'NUM_VAL_WORKERS': 4,\n   'MAX_TOKEN_COUNT': 165,\n   'SEED': 42,\n   'WEIGHTS': [0.54238067,\n    0.38692124,\n    2.28156376,\n    3.17629465,\n    0.36239603,\n    0.89006059,\n    0.87405231,\n    1.44634846,\n    0.27533886,\n    0.31700788,\n    0.97389395,\n    0.46999331,\n    2.6127585,\n    1.34618582,\n    0.41359411,\n    0.70227324,\n    0.26240447,\n    1.32057903,\n    0.82508163,\n    0.52087147],\n   'WEIGHTS_APPLIED': False,\n   'CRITERION': [BCEWithLogitsLoss()],\n   'DATA': 'Premise + Stance + Conclusion',\n   'OPTIMIZER': 'AdamW',\n   'ACCUMULATE_GRAD_BATCHES': 1,\n   'PATIENCE': 3,\n   'VAL_CHECK_INTERVAL': 300,\n   'MAX_THRESHOLD_METRIC': 'custom',\n   'EARLY_STOPPING_METRIC': 'avg_val_loss',\n   'EARLY_STOPPING_MODE': 'min',\n   'ACTIVATION': None,\n   'DROPOUT': None,\n   'HIDDEN_LAYERS': None,\n   'TRAIN_PATH': 'data_training_individual_v2_300_500.csv',\n   'VALIDATION_PATH': None,\n   'TEST_PATH': 'data_test_individual_v2_500.csv'},\n  'ID': 'HCV-16',\n  'MODEL_CHECKPOINT': './checkpoints/HCV-16-roberta-base-BS_8-LR_2e-05-HL_None-DROPOUT_None.ckpt'},\n {'PARAMS': {'MODEL_PATH': 'danschr/roberta-large-BS_16-EPOCHS_8-LR_5e-05-ACC_GRAD_2-MAX_LENGTH_165',\n   'BATCH_SIZE': 8,\n   'LR': 2e-05,\n   'EPOCHS': 20,\n   'DEVICE': device(type='cuda'),\n   'NUM_TRAIN_WORKERS': 4,\n   'NUM_VAL_WORKERS': 4,\n   'MAX_TOKEN_COUNT': 165,\n   'SEED': 42,\n   'WEIGHTS': [0.54238067,\n    0.38692124,\n    2.28156376,\n    3.17629465,\n    0.36239603,\n    0.89006059,\n    0.87405231,\n    1.44634846,\n    0.27533886,\n    0.31700788,\n    0.97389395,\n    0.46999331,\n    2.6127585,\n    1.34618582,\n    0.41359411,\n    0.70227324,\n    0.26240447,\n    1.32057903,\n    0.82508163,\n    0.52087147],\n   'WEIGHTS_APPLIED': False,\n   'CRITERION': [BCEWithLogitsLoss()],\n   'DATA': 'Premise + Stance + Conclusion',\n   'OPTIMIZER': 'AdamW',\n   'ACCUMULATE_GRAD_BATCHES': 1,\n   'PATIENCE': 3,\n   'VAL_CHECK_INTERVAL': 300,\n   'MAX_THRESHOLD_METRIC': 'custom',\n   'EARLY_STOPPING_METRIC': 'avg_val_loss',\n   'EARLY_STOPPING_MODE': 'min',\n   'ACTIVATION': None,\n   'DROPOUT': None,\n   'HIDDEN_LAYERS': None,\n   'TRAIN_PATH': 'data_training_individual_v2_300_500.csv',\n   'VALIDATION_PATH': None,\n   'TEST_PATH': 'data_test_individual_v2_500.csv'},\n  'ID': 'HCV-17',\n  'MODEL_CHECKPOINT': './checkpoints/HCV-17-roberta-base-BS_8-LR_2e-05-HL_None-DROPOUT_None.ckpt'}]"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenating relevant information into one Ensemble_list: Parameters, Id, and Path to Checkpoint.\n",
    "ENSEMBLE_LIST = []\n",
    "for param, id, mc in zip(PARAMS_LIST, PARAMS_ENSEMBLE[\"IDS\"], PARAMS_ENSEMBLE[\"MODEL_CHECKPOINTS\"]):\n",
    "    ENSEMBLE_LIST.append({\"PARAMS\":param, \"ID\":id,\"MODEL_CHECKPOINT\":mc})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predict Test File"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load test-data from Path specified in Params above\n",
    "Prepare text column by concatenating premise, stance and conclusion\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_df_input = pd.read_csv(PARAMS_ENSEMBLE[\"TEST_PATH\"], sep='\\t')\n",
    "\n",
    "test_df_input[\"text\"] = test_df_input[\"Premise\"]+\" \" + test_df_input[\"Stance\"]+ \" \" + test_df_input[\"Conclusion\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def predict_unseen_data(trained_model, data):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    trained_model = trained_model.to(device)\n",
    "\n",
    "    test_dataset = BertDataset(\n",
    "        data=data,\n",
    "        tokenizer=TOKENIZER,\n",
    "        max_token_count=PARAMS[\"MAX_TOKEN_COUNT\"],\n",
    "    )\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for item in tqdm(test_dataset):\n",
    "        _, prediction = trained_model(\n",
    "            item[\"input_ids\"].unsqueeze(dim=0).to(device),\n",
    "            item[\"attention_mask\"].unsqueeze(dim=0).to(device)\n",
    "        )\n",
    "        predictions.append(prediction.flatten())\n",
    "\n",
    "    predictions = torch.stack(predictions).detach().cpu()\n",
    "\n",
    "    return predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over elements in Ensemble_List and get predictions from each model. Collect them in predictions [] list.\n",
    "predictions = []\n",
    "for idx, elem in enumerate(ENSEMBLE_LIST):\n",
    "    print(f\"Starting with model {elem['MODEL_CHECKPOINT']}\")\n",
    "    PARAMS = elem[\"PARAMS\"]\n",
    "    trained_model = BertFineTunerPl.load_from_checkpoint(\n",
    "        elem[\"MODEL_CHECKPOINT\"],\n",
    "        params=PARAMS,\n",
    "        label_columns=LABEL_COLUMNS,\n",
    "        n_classes=len(LABEL_COLUMNS)\n",
    "    )\n",
    "    trained_model.eval()\n",
    "    trained_model.freeze()\n",
    "    print(f\"With Tokenizer {PARAMS['MODEL_PATH']}\")\n",
    "    TOKENIZER = AutoTokenizer.from_pretrained(PARAMS[\"MODEL_PATH\"])\n",
    "    pred = predict_unseen_data(trained_model=trained_model, data=test_df_input)\n",
    "    predictions.append(pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For each sample we now have 12 predictions. So we take the average by first stacking them together"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions = torch.stack(predictions).numpy()\n",
    "predictions_avg = np.mean(predictions, axis=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Binarize the Output with the optimal decision threshold (previously defined)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "upper, lower = 1, 0\n",
    "\n",
    "# Use optimal decision threshold.\n",
    "y_pred = np.where(predictions_avg > PARAMS_ENSEMBLE[\"ENSEMBLE_THRESHOLD\"], upper, lower)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create test-file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prediction_dictionary = {}\n",
    "prediction_dictionary[\"Argument ID\"] = test_df_input[\"Argument ID\"]\n",
    "for idx, l_name in enumerate(LABEL_COLUMNS):\n",
    "    prediction_dictionary[l_name]=y_pred[:,idx]\n",
    "\n",
    "test_prediction_df = pd.DataFrame(prediction_dictionary)\n",
    "test_prediction_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_prediction_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [4]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtest_prediction_df\u001B[49m\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msubmissions/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mRUN_ID\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m-submission_validation.tsv\u001B[39m\u001B[38;5;124m\"\u001B[39m, sep\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'test_prediction_df' is not defined"
     ]
    }
   ],
   "source": [
    "test_prediction_df.to_csv(f\"./submission_test.tsv\", sep=\"\\t\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
