{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Reproduce the Competition Results: Predict\n",
    "Please take a look in the Readme to setup the data and checkpoints"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# If on Colab\n",
    "!pip install -q pytorch-lightning==1.6.4 neptune-client transformers sentencepiece"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": "42"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import pickle\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "COLAB = True\n",
    "\n",
    "pl.seed_everything(RANDOM_SEED)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Connect to GoogleDrive if running on Colab"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    import os\n",
    "    os.getcwd()\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cd ./drive/MyDrive/human_value/human_value_detector"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!git pull"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Pytorch-Lightning Model and set paths to model-checkpoints.\n",
    "These Models are ensembled together to make the prediction on the test-file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from data_modules.BertDataModule import BertDataset\n",
    "from models.BertFineTunerPl import BertFineTunerPl"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we define the Models that we want to ensemble. Download the Models used for the submission and place them in the checkpoint folder. Here you can then specify the path in to them in order to reproduce the results.  (If you want to ensemble different combinations just select them here. If you have own models trained then you can place them here too, but you need to ensure the params are loaded (see below))."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "PARAMS_ENSEMBLE = {\n",
    "    \"MODEL_CHECKPOINTS\": ['./checkpoints/HCV-409-microsoft-deberta-large-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-408-microsoft-deberta-large-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-406-microsoft-deberta-large-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-402-danschr-roberta-large-BS_16-EPOCHS_8-LR_5e-05-ACC_GRAD_2-MAX_LENGTH_165-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-403-danschr-roberta-large-BS_16-EPOCHS_8-LR_5e-05-ACC_GRAD_2-MAX_LENGTH_165-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-405-danschr-roberta-large-BS_16-EPOCHS_8-LR_5e-05-ACC_GRAD_2-MAX_LENGTH_165-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-364-microsoft-deberta-large-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-366-microsoft-deberta-large-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-368-microsoft-deberta-large-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-371-danschr-roberta-large-BS_16-EPOCHS_8-LR_5e-05-ACC_GRAD_2-MAX_LENGTH_165-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-372-danschr-roberta-large-BS_16-EPOCHS_8-LR_5e-05-ACC_GRAD_2-MAX_LENGTH_165-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt',\n",
    "                          './checkpoints/HCV-375-danschr-roberta-large-BS_16-EPOCHS_8-LR_5e-05-ACC_GRAD_2-MAX_LENGTH_165-BS_8-LR_2e-05-HL_None-DROPOUT_None-SL_None.ckpt'\n",
    "                          ],\n",
    "    \"DESCRIPTION\":\"FULL #3xDebL_F1 3EP 3xdanRobL_F1 3EP 3xDebL_Loss 3EP 3xdanRobL_Loss 3EP\",\n",
    "    \"TEST_PATH\" : \"./data/arguments-test.tsv\",\n",
    "    \"MAX_THRESHOLD_METRIC\": \"custom\",\n",
    "    \"ENSEMBLE\": \"EN\",\n",
    "    \"ENSEMBLE_THRESHOLD\":0.26, #You find the optimal threshold in the paper (or calculate in the ensemble_eval_and_predict.ipynb)\n",
    "    \"LABEL_COLUMNS\":['Self-direction: thought',\n",
    "                     'Self-direction: action',\n",
    "                     'Stimulation',\n",
    "                     'Hedonism',\n",
    "                     'Achievement',\n",
    "                     'Power: dominance',\n",
    "                     'Power: resources',\n",
    "                     'Face',\n",
    "                     'Security: personal',\n",
    "                     'Security: societal',\n",
    "                     'Tradition',\n",
    "                     'Conformity: rules',\n",
    "                     'Conformity: interpersonal',\n",
    "                     'Humility',\n",
    "                     'Benevolence: caring',\n",
    "                     'Benevolence: dependability',\n",
    "                     'Universalism: concern',\n",
    "                     'Universalism: nature',\n",
    "                     'Universalism: tolerance',\n",
    "                     'Universalism: objectivity']\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We extract the identifier e.g \"HCV-409\" from the checkpoint paths."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the Identifier within Model-Path. (Used to get corresponding PARAMS File)\n",
    "NAME = \"\"\n",
    "ids = []\n",
    "for elem in PARAMS_ENSEMBLE[\"MODEL_CHECKPOINTS\"]:\n",
    "    text_list = elem.split(\"checkpoints/\")[1]\n",
    "    text_list = text_list.split(\"-\")\n",
    "    id = text_list[0]+\"-\" + text_list[1]\n",
    "    ids.append(id)\n",
    "    NAME= NAME + \"_\" + id\n",
    "    print(text_list[0]+\"-\" + text_list[1])\n",
    "NAME = PARAMS_ENSEMBLE[\"ENSEMBLE\"]+\"_\"+NAME[1:]\n",
    "\n",
    "PARAMS_ENSEMBLE[\"IDS\"] = ids\n",
    "LABEL_COLUMNS = PARAMS_ENSEMBLE[\"LABEL_COLUMNS\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Ensemble List"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Take IDs that have been generated and get the params_file with the same id"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Loading the parameters for each model\n",
    "PARAMS_LIST = []\n",
    "for id in PARAMS_ENSEMBLE[\"IDS\"]:\n",
    "    with open(f'./checkpoints/{id}_PARAMS.pkl', 'rb') as f:\n",
    "        loaded_dict = pickle.load(f)\n",
    "        PARAMS_LIST.append(loaded_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We group together the checkpoint and parameters in a list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "\u001B[31m╭─\u001B[0m\u001B[31m──────────────────────────────\u001B[0m\u001B[31m \u001B[0m\u001B[1;31mTraceback \u001B[0m\u001B[1;2;31m(most recent call last)\u001B[0m\u001B[31m \u001B[0m\u001B[31m───────────────────────────────\u001B[0m\u001B[31m─╮\u001B[0m\n\u001B[31m│\u001B[0m \u001B[33mC:\\Users\\dschr\\AppData\\Local\\Temp\\ipykernel_31284\\4058388596.py\u001B[0m:\u001B[94m3\u001B[0m in \u001B[92m<cell line: 3>\u001B[0m              \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[3;31m[Errno 2] No such file or directory: \u001B[0m                                                            \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[3;31m'C:\\\\Users\\\\dschr\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_31284\\\\4058388596.py'\u001B[0m                         \u001B[31m│\u001B[0m\n\u001B[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001B[0m\n\u001B[1;91mNameError: \u001B[0mname \u001B[32m'PARAMS_LIST'\u001B[0m is not defined\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\dschr\\AppData\\Local\\Temp\\ipykernel_31284\\4058388596.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 3&gt;</span>              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\dschr\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_31284\\\\4058388596.py'</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'PARAMS_LIST'</span> is not defined\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Concatenating relevant information into one Ensemble_list: Parameters, Id, and Path to Checkpoint.\n",
    "ENSEMBLE_LIST = []\n",
    "for param, id, mc in zip(PARAMS_LIST, PARAMS_ENSEMBLE[\"IDS\"], PARAMS_ENSEMBLE[\"MODEL_CHECKPOINTS\"]):\n",
    "    ENSEMBLE_LIST.append({\"PARAMS\":param, \"ID\":id,\"MODEL_CHECKPOINT\":mc})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predict The Submission Tetst File"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load test-data from Path specified in Params above (./data/arguments-test.tsv) for official submission.  But can be other test-files if you have some).\n",
    "Prepare text column by concatenating premise, stance and conclusion\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_df_input = pd.read_csv(PARAMS_ENSEMBLE[\"TEST_PATH\"], sep='\\t')\n",
    "test_df_input[\"text\"] = test_df_input[\"Premise\"]+\" \" + test_df_input[\"Stance\"]+ \" \" + test_df_input[\"Conclusion\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict_unseen_data(trained_model, data):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    trained_model = trained_model.to(device)\n",
    "\n",
    "    test_dataset = BertDataset(\n",
    "        data=data,\n",
    "        tokenizer=TOKENIZER,\n",
    "        max_token_count=PARAMS[\"MAX_TOKEN_COUNT\"],\n",
    "    )\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for item in tqdm(test_dataset):\n",
    "        _, prediction = trained_model(\n",
    "            item[\"input_ids\"].unsqueeze(dim=0).to(device),\n",
    "            item[\"attention_mask\"].unsqueeze(dim=0).to(device)\n",
    "        )\n",
    "        predictions.append(prediction.flatten())\n",
    "\n",
    "    predictions = torch.stack(predictions).detach().cpu()\n",
    "\n",
    "    return predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We iterate over the Models in the Ensemble List and get the predictions for the test-dataset for each model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over elements in Ensemble_List and get predictions from each model. Collect them in predictions [] list.\n",
    "predictions = []\n",
    "for idx, elem in enumerate(ENSEMBLE_LIST):\n",
    "    print(f\"Starting with model {elem['MODEL_CHECKPOINT']}\")\n",
    "    PARAMS = elem[\"PARAMS\"]\n",
    "    trained_model = BertFineTunerPl.load_from_checkpoint(\n",
    "        elem[\"MODEL_CHECKPOINT\"],\n",
    "        params=PARAMS,\n",
    "        label_columns=LABEL_COLUMNS,\n",
    "        n_classes=len(LABEL_COLUMNS)\n",
    "    )\n",
    "    trained_model.eval()\n",
    "    trained_model.freeze()\n",
    "    print(f\"With Tokenizer {PARAMS['MODEL_PATH']}\")\n",
    "    TOKENIZER = AutoTokenizer.from_pretrained(PARAMS[\"MODEL_PATH\"])\n",
    "    pred = predict_unseen_data(trained_model=trained_model, data=test_df_input)\n",
    "    predictions.append(pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For each sample we now have 12 predictions. We stack the predictions together and then take the average."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions = torch.stack(predictions).numpy()\n",
    "predictions_avg = np.mean(predictions, axis=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Binarize the Output with the optimal decision threshold (previously defined)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "upper, lower = 1, 0\n",
    "\n",
    "# Use optimal decision threshold.\n",
    "y_pred = np.where(predictions_avg > PARAMS_ENSEMBLE[\"ENSEMBLE_THRESHOLD\"], upper, lower)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create test-file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prediction_dictionary = {}\n",
    "prediction_dictionary[\"Argument ID\"] = test_df_input[\"Argument ID\"]\n",
    "for idx, l_name in enumerate(LABEL_COLUMNS):\n",
    "    prediction_dictionary[l_name]=y_pred[:,idx]\n",
    "\n",
    "test_prediction_df = pd.DataFrame(prediction_dictionary)\n",
    "test_prediction_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_prediction_df.to_csv(f\"./submission_test.tsv\", sep=\"\\t\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
